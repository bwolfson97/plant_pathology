{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "\n",
    "> This module contains a script to train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from plant_pathology.config import DATA_PATH\n",
    "from plant_pathology.dataset import get_dls\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sys import exit\n",
    "from typing import Callable, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "from fastai.callback.wandb import WandbCallback, wandb\n",
    "from fastai.vision.all import *\n",
    "from wwf.vision.timm import timm_learner\n",
    "\n",
    "from plant_pathology.dataset import get_dls_all_in_1\n",
    "from plant_pathology.evaluate import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Model on Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def timm_or_fastai_arch(arch: str) -> Tuple[Union[Callable, str], Callable]:\n",
    "    \"\"\"Check if `arch` is a fast.ai or timm architecture and return appropriate functions.\"\"\"\n",
    "    try:  # Check if fastai arch\n",
    "        model = globals()[arch]\n",
    "        learner_func = cnn_learner\n",
    "    except KeyError:  # Must be timm arch\n",
    "        model = arch\n",
    "        learner_func = timm_learner\n",
    "    return model, learner_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train(\n",
    "    data_path: Path,\n",
    "    epochs: int = 1,\n",
    "    lr: Union[float, str] = 3e-4,\n",
    "    frz: int = 1,\n",
    "    pre: int = 800,\n",
    "    re: int = 256,\n",
    "    bs: int = 200,\n",
    "    fold: int = 4,\n",
    "    smooth: bool = False,\n",
    "    arch: str = \"resnet18\",\n",
    "    dump: bool = False,\n",
    "    log: bool = False,\n",
    "    mixup: float = 0.0,\n",
    "    fp16: bool = False,\n",
    "    dls: DataLoaders = None,\n",
    "    save: bool = False,\n",
    "    pseudo: Path = None,\n",
    ") -> Learner:\n",
    "    \"\"\"\"Train a learner on training CSV (w/folds) at `data_path`.\"\"\"\n",
    "    # Build DataLoaders\n",
    "    if dls is None:\n",
    "        dls = get_dls_all_in_1(\n",
    "            data_path=data_path,\n",
    "            presize=pre,\n",
    "            resize=re,\n",
    "            bs=bs,\n",
    "            val_fold=fold,\n",
    "            pseudo_labels_path=pseudo,\n",
    "        )\n",
    "\n",
    "    # Initialize wandb logging\n",
    "    if log:\n",
    "        wandb.init(project=\"plant-pathology\")\n",
    "\n",
    "    # Get correct loss, depending on if using label smoothing\n",
    "    if smooth:\n",
    "        loss_func = LabelSmoothingCrossEntropyFlat()\n",
    "    else:\n",
    "        loss_func = CrossEntropyLossFlat()\n",
    "\n",
    "    # Get model and learner_func\n",
    "    m, learner_func = timm_or_fastai_arch(arch)\n",
    "\n",
    "    # Build callbacks\n",
    "    cbs = []\n",
    "    if save or log:\n",
    "        cbs.append(SaveModelCallback(\"roc_auc_score\", fname=f\"model_val_on_{fold}\"))\n",
    "    if log:\n",
    "        cbs.append(WandbCallback())\n",
    "    if mixup:\n",
    "        cbs.append(MixUp(mixup))\n",
    "\n",
    "    # Build learner\n",
    "    print(f\"# train exs: {len(dls.train_ds)}, val exs: {len(dls.valid_ds)}\")\n",
    "    learn = learner_func(\n",
    "        dls, m, loss_func=loss_func, metrics=[accuracy, RocAuc()], cbs=cbs\n",
    "    )\n",
    "\n",
    "    # If we just want to print architecture, do that and exit\n",
    "    if dump:\n",
    "        print(learn.model)\n",
    "        exit()\n",
    "\n",
    "    # If we are just running learning rate finder, do that and exit\n",
    "    if lr == \"find\":\n",
    "        learn.lr_find()\n",
    "        exit()\n",
    "\n",
    "    # Use mixed-precision training if desired\n",
    "    if fp16:\n",
    "        learn.to_fp16()\n",
    "\n",
    "    # Train only head at first\n",
    "    learn.freeze()\n",
    "    learn.fit_one_cycle(frz, lr)\n",
    "\n",
    "    # Train all layers, using discriminative learning rate\n",
    "    learn.unfreeze()\n",
    "    learn.fit_one_cycle(epochs, slice(lr / 100, lr / 2))\n",
    "\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train exs: 1457, val exs: 364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.950449</td>\n",
       "      <td>1.180670</td>\n",
       "      <td>0.508242</td>\n",
       "      <td>0.749386</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "learn = train(DATA_PATH, epochs=0, lr=0.001, bs=256, log=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Using Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def softmax_RocAuc(logits, labels):\n",
    "    \"\"\"Compute RocAuc, first taking softmax of `logits`.\"\"\"\n",
    "    probs = logits.softmax(-1)\n",
    "    return RocAuc()(probs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.4090,  1.6367, -0.1251,  1.2801],\n",
       "         [-1.3648,  0.4716, -2.0059, -0.8697]]),\n",
       " torch.Size([4, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "preds = torch.randn(2, 4)\n",
    "labels = tensor([1, 2, 3, 4]).unsqueeze(-1)\n",
    "preds, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def train_cv(\n",
    "    path:     Param(\"Path to data dir\", Path),\n",
    "    epochs:   Param(\"Number of unfrozen epochs\", int) = 1,\n",
    "    lr:       Param(\"Initial learning rate\", float) = 3e-4,\n",
    "    frz:      Param(\"Number of frozen epochs\", int) = 1,\n",
    "    pre:      Param(\"Image presize\", int, nargs=\"+\") = (682, 1024),\n",
    "    re:       Param(\"Image resize\", int) = 256,\n",
    "    bs:       Param(\"Batch size\", int) = 256,\n",
    "    smooth:   Param(\"Label smoothing?\", store_true) = False,\n",
    "    arch:     Param(\"Architecture\", str) = \"resnet18\",\n",
    "    dump:     Param(\"Don't train, just print model\", store_true) = False,\n",
    "    log:      Param(\"Log w/ W&B\", store_true) = False,\n",
    "    save:     Param(\"Save model based on RocAuc\", store_true) = False,\n",
    "    mixup:    Param(\"Mixup (0.4 is good)\", float) = 0.0,\n",
    "    tta:      Param(\"Test-time augmentation\", store_true) = False,\n",
    "    fp16:     Param(\"Mixed-precision training\", store_true) = False,\n",
    "    do_eval:  Param(\"Evaluate model and save predictions CSV\", store_true) = False,\n",
    "    val_fold: Param(\"Don't do cross-validation, just do 1 fold\", int) = None,\n",
    "    pseudo:   Param(\"Path to pseudo labels to train on\", Path) = None,\n",
    "    export:   Param(\"Export learner(s) to export_val_on_{fold}.pkl\", store_true) = False,\n",
    "):\n",
    "    \"\"\"Train models using 5-fold cross-validation.\"\"\"\n",
    "    # Print parameters this was called with\n",
    "    print(locals())\n",
    "\n",
    "    # Do 5-fold cross-validation\n",
    "    scores = []\n",
    "    for fold in range(5):\n",
    "        if val_fold is not None:\n",
    "            # User passed specific fold, so don't do CV. Just do single run w/val_fold.\n",
    "            fold = val_fold\n",
    "\n",
    "        print(f\"\\nTraining on fold {fold}\")\n",
    "        learn = train(\n",
    "            data_path=path,\n",
    "            epochs=epochs,\n",
    "            lr=lr,\n",
    "            frz=frz,\n",
    "            pre=pre,\n",
    "            re=re,\n",
    "            bs=bs,\n",
    "            smooth=smooth,\n",
    "            arch=arch,\n",
    "            dump=dump,\n",
    "            log=log,\n",
    "            fold=fold,\n",
    "            mixup=mixup,\n",
    "            fp16=fp16,\n",
    "            save=save,\n",
    "            pseudo=pseudo,\n",
    "        )\n",
    "\n",
    "        # Bug when doing tta w/Mixup\n",
    "        if hasattr(learn, \"mixup\") and tta:\n",
    "            learn.remove_cb(MixUp)\n",
    "\n",
    "        # Add final record from this split to scores\n",
    "        if tta and len(learn.dls.valid_ds) != 0:  # There is a validation set\n",
    "            # Must get final metrics manually if using TTA\n",
    "            preds, lbls = learn.tta()\n",
    "            res = [f(preds, lbls) for f in [learn.loss_func, accuracy, softmax_RocAuc]]\n",
    "        else:\n",
    "            res = learn.final_record\n",
    "        scores.append(res)\n",
    "\n",
    "        # Create submission file\n",
    "        if do_eval:\n",
    "            print(\"Evaluating\")\n",
    "            evaluate(\n",
    "                learn,\n",
    "                path=path / \"test.csv\",\n",
    "                name=f\"predictions_fold_{fold}.csv\",\n",
    "                tta=tta,\n",
    "            )\n",
    "\n",
    "        if export:\n",
    "            learn.export(f\"export_val_on_{fold}.pkl\")\n",
    "\n",
    "        # Delete learner to avoid OOM\n",
    "        del learn\n",
    "\n",
    "        # If only training on single fold, break\n",
    "        if val_fold is not None:\n",
    "            break\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    print(f\"Scores: {scores}\\n\")\n",
    "\n",
    "    # Print average stats across folds, if trained on multiple folds\n",
    "    if val_fold is None:\n",
    "        print(f\"Mean: {scores.mean(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = np.ones((5, 4))\n",
    "scores.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'path': Path('../data'), 'epochs': 0, 'lr': 0.02, 'frz': 1, 'pre': 64, 're': 64, 'bs': 512, 'smooth': False, 'arch': 'resnet18', 'dump': False, 'log': False, 'save': False, 'mixup': 0.4, 'tta': True, 'fp16': True, 'do_eval': True, 'val_fold': 4, 'pseudo': None, 'export': True}\n",
      "\n",
      "Training on fold 4\n",
      "# train exs: 1457, val exs: 364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>roc_auc_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.366835</td>\n",
       "      <td>5.700662</td>\n",
       "      <td>0.151099</td>\n",
       "      <td>0.559646</td>\n",
       "      <td>00:24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='0' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandon/miniconda3/lib/python3.8/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='0' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      \n",
       "    </div>\n",
       "    \n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brandon/miniconda3/lib/python3.8/site-packages/fastprogress/fastprogress.py:74: UserWarning: Your generator is empty.\n",
      "  warn(\"Your generator is empty.\")\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores: [[1.50433147 0.13736264 0.57773272]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#slow\n",
    "#hide\n",
    "train_cv(DATA_PATH, 0, 2e-2, pre=64, re=64, bs=512, fp16=True, val_fold=4, tta=True, mixup=0.4, do_eval=True, export=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#hide\n",
    "# Check predictions CSV was saved\n",
    "preds_path = Path(\"predictions_fold_4.csv\")\n",
    "assert preds_path.exists(), \"Predictions CSV not saved properly\"\n",
    "preds_path.unlink()\n",
    "\n",
    "# Check Learner was exported properly\n",
    "export_path = Path(\"export_val_on_4.pkl\")\n",
    "assert export_path.exists(), \"Learner not exported properly\"\n",
    "export_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 03_train.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
